[DEAFULT]
# 模型名称
model_name = textrnn_pytorch
# 数据目录
data_path =
# 输出路径,用于存储模型文件等
output_path =
# label映射文件
label2idx_path =
# 预训练的词向量的路径
pretrain_embedding =
# 停用词表文件路径
stopwords_path =
# checkpoint 模型的存储路径
ckpt_model_path =
# 序列长度,每句话处理成的长度(短填长切)
sequence_length = 512
# 样本的类别数,二分类时置为1,多分类时置为实际类别数,多标签分类时设置为0
num_labels = 14
# 字\词向量维度
embedding_dim = 300
# 字典大小,词表大小,在运行时赋值
vocab_size = 10000
# lstm隐藏层大小
hidden_size = 256
#lstm层数
num_layers = 1
# 保留神经元的比例,随机失活
dropout_keep_prob = 0.8
# 学习速率
learning_rate = 0.001
# 全样本迭代次数
num_epochs = 1
# 批样本大小,mini-batch大小
batch_size = 64
# 迭代多少步保存一次模型文件
checkpoint_every = 1000
# 迭代多少步验证一次模型
eval_every_step = 1000
# 若超过1000batch效果还没提升，则提前结束训练
require_improvement = 1000


[THUC_NEWS]
model_name = textrnn_pytorch
data_path = /data/work/dl_project/data/corpus/thuc_news
output_path = /data/work/dl_project/data/model/thuc_news/textrnn_pytorch_output
label2idx_path = /data/work/dl_project/data/corpus/thuc_news/label2idx.json
sequence_length = 512
num_labels = 14
embedding_dim = 300
vocab_size = 10000
hidden_size = 256
num_layers = 1
dropout_keep_prob = 0.8
learning_rate = 0.001
num_epochs = 1
batch_size = 64
eval_every_step = 1000
require_improvement = 1000
