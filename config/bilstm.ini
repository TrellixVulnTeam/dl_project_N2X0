[DEAFULT]
# 模型名称
model_name = bilstm
# 数据目录(语料文件\映射文件)
data_path =
# 输出目录(模型文件\日志文件\临时文件等)
output_path =
# 模型目录
ckpt_model_path =
# 词表映射文件
word2idx_file =
# label映射文件
label2idx_file =
# 预训练embedding文件
pretrain_embedding_file =
# 停用词文件
stopwords_file =
# 类别数,二分类时置为1,多分类时置为实际类别数
num_labels =
# 字\词向量维度
embedding_dim = 300
# 字典(词表)大小
vocab_size = 20000
# 序列长度,每句话处理成的长度(短填长切)
sequence_length = 512
# 学习速率
learning_rate = 0.001
# 学习率衰减系数
learning_decay_rate = 0.99
# 迭代多少轮就衰减的度量值,可叫作衰减速度
learning_decay_steps = 10000
# lstm的隐层大小，列表对象，支持多层lstm，只要在列表中添加相应的层对应的隐层大小
hidden_sizes = [256,256]
# 保留神经元的比例,随机失活
dropout_keep_prob = 1.0
# 优化器
optimization = adam
# L2正则化的系数，主要对全连接层的参数正则化
l2_reg_lambda = 0.0
# 梯度的最大范数
max_grad_norm = 5.0
# 全样本迭代次数
num_epochs = 1
# 批样本(mini-batch)大小
batch_size = 64
# 迭代多少step验证一次模型
eval_every_step = 100
# 迭代多少step保存一次模型
save_checkpoints_steps = 100
# 若超过100个batch(epoch)验证集指标还没提升，则提前结束训练
require_improvement = 100



[THUC_NEWS]
data_path = /data/work/dl_project/data/corpus/thuc_news
output_path = /data/work/dl_project/data/model/thuc_news/bilstm_output
label2idx_file = /data/work/dl_project/data/corpus/thuc_news/label2idx.json
pretrain_embedding_file = /data/work/dl_project/data/pretrained_embedding/sgns.sogou.char
sequence_length = 512
num_labels = 14
embedding_dim = 300
vocab_size = 8000
hidden_sizes = [256]
dropout_keep_prob = 0.8
optimization = adam
learning_rate = 0.001
l2_reg_lambda = 0.0
num_epochs = 1
batch_size = 64
eval_every_step = 500
model_name = bilstm
