[DEAFULT]
# 模型名称
model_name = textrnn
# 数据目录(语料文件\映射文件)
data_path =
# 输出目录(模型文件\日志文件\临时文件等)
output_path =
# 预训练bert路径
init_checkpoint_path =
# 模型目录
ckpt_model_path =
# 词表映射文件
word2idx_file =
# label映射文件
label2idx_file =
# 预训练embedding文件
pretrain_embedding_file =
# 停用词文件
stopwords_file =
# 类别数,二分类时置为1,多分类时置为实际类别数
num_labels =
# 字\词向量维度
embedding_dim = 300
# 字典(词表)大小
vocab_size = 20000
# 序列长度,每句话处理成的长度(短填长切)
sequence_length = 512
# 学习速率
learning_rate = 0.001
# 学习率衰减系数
learning_decay_rate = 0.99
# 迭代多少轮就衰减的度量值,可叫作衰减速度
learning_decay_steps = 10000
# 输出层(全连接层)神经元个数,如果有设置全连接层,可不设置输出层神经元
hidden_size = 256
# 保留神经元的比例,随机失活
dropout_keep_prob = 1.0
# 优化器
optimization = adam
# L2正则化的系数，主要对全连接层的参数正则化
l2_reg_lambda = 0.0
# 梯度的最大范数
max_grad_norm = 5.0
# 全样本迭代次数
num_epochs = 1
# 批样本(mini-batch)大小
batch_size = 64
# 迭代多少step验证一次模型
eval_every_step = 100
# 迭代多少step保存一次模型
save_checkpoints_steps = 100
# 若超过100个batch(epoch)验证集指标还没提升，则提前结束训练
require_improvement = 100


[THUC_NEWS]
data_path = /data/work/dl_project/data/corpus/thuc_news
label2idx_path = /data/work/dl_project/data/corpus/thuc_news/label2idx.json
pretrain_embedding =
output_path = /data/work/dl_project/data/model/thuc_news/textrnn_output
ckpt_model_path =
sequence_length = 512
num_labels = 14
embedding_dim = 300
vocab_size = 8000
rnn_gate = lstm
num_layers = 2
hidden_size = 128
is_training = True
dropout_keep_prob = 0.8
optimization = adam
learning_rate = 0.001
learning_decay_rate = 0.99
learning_decay_steps = 10000
l2_reg_lambda = 0.0
max_grad_norm = 5.0
num_epochs = 1
train_batch_size = 32
eval_batch_size = 128
test_batch_size = 128
eval_every_step = 500
model_name = textrnn
