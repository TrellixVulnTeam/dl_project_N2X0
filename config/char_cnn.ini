[DEAFULT]
# 数据目录
data_path =
# label映射文件
label2idx_path =
# 词向量的路径
pretrain_embedding =
# 输出路径,用于存储模型文件等
output_path =
# pb 模型的存储路径
pb_model_path =
# checkpoint 模型的存储路径
ckpt_model_path =
# 停用词表文件路径
stopwords_path =
# 序列长度
sequence_length = 512
# 样本的类别数,二分类时置为1,多分类时置为实际类别数,多标签分类时设置为0
num_labels = 14
# embedding层维度大小
embedding_dim = 300
# 词汇表大小
vocab_size = 10000
# 卷积层尺寸, 该列表中子列表的三个元素分别是卷积核的数量，卷积核的高度，池化的尺寸
conv_layers_size = [[256, 7, 3],[256, 7, 3],[256, 3, None],[256, 3, None],[256, 3, None],[256, 3, 3]]
# 全连接层神经元尺寸, 该列表中每个元素为全连接层的一层的神经元个数
fc_layers_size = [1024,1024,256]
# 输出层神经元,如果有设置全连接层,可不设置输出层神经元
output_size = 256
# char_cnn模型是否使用大参数,如果设置了conv_layers_size, fc_layers_size,该参数可不选
large_params = False
is_training = True
# 保留神经元的比例
dropout_keep_prob = 0.8
# 优化算法
optimization = adam
# 学习速率
learning_rate = 0.001
learning_decay_rate = 0.99
learning_decay_steps = 10000
# L2正则化的系数，主要对全连接层的参数正则化
l2_reg_lambda = 0.0
# 梯度阶段临界值
max_grad_norm = 5.0
# 全样本迭代次数
num_epochs = 1
# 训练集批样本大小
train_batch_size = 64
# 验证集批样本大小
eval_batch_size = 128
# 测试集批样本大小
test_batch_size = 128
# 迭代多少步保存一次模型文件
checkpoint_every = 1000
# 迭代多少步验证一次模型
eval_every_step = 1000
# 模型名称
model_name = char_cnn



[THUC_NEWS]
data_path = /data/work/dl_project/data/corpus/thuc_news
label2idx_path = /data/work/dl_project/data/corpus/thuc_news/label2idx.json
pretrain_embedding =
output_path = /data/work/dl_project/data/model/thuc_news/textcnn_output
ckpt_model_path =
sequence_length = 512
num_labels = 14
embedding_dim = 300
vocab_size = 8000
conv_layers_size = [[256, 7, 3],[256, 7, 3],[256, 3, 3]]
fc_layers_size = [256,128]
is_training = True
dropout_keep_prob = 0.8
optimization = adam
learning_rate = 0.001
learning_decay_rate = 0.99
learning_decay_steps = 10000
l2_reg_lambda = 0.0
max_grad_norm = 5.0
num_epochs = 1
train_batch_size = 64
eval_batch_size = 128
test_batch_size = 128
eval_every_step = 500
model_name = char_cnn