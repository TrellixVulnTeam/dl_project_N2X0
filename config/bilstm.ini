[DEAFULT]
# 数据目录
data_path =
# label映射文件
label2idx_path =
# 词向量的路径
pretrain_embedding =
# 输出路径,用于存储模型文件等
output_path =
# pb 模型的存储路径
pb_model_path =
# checkpoint 模型的存储路径
ckpt_model_path =
# 停用词表文件路径
stopwords_path =
# 序列长度
sequence_length = 512
# 样本的类别数,二分类时置为1,多分类时置为实际类别数,多标签分类时设置为0
num_labels = 14
# embedding层维度大小
embedding_dim = 300
# 词汇表大小
vocab_size = 10000
# lstm的隐层大小，列表对象，支持多层lstm，只要在列表中添加相应的层对应的隐层大小
hidden_sizes = [128]
# lstm层的输出非线性映射的神经元大小(可不比设置)
output_size = 128
is_training = True
# 保留神经元的比例
dropout_keep_prob = 0.8
# 优化算法
optimization = adam
# 学习速率
learning_rate = 0.001
learning_decay_rate = 0.99
learning_decay_steps = 10000
# L2正则化的系数，主要对全连接层的参数正则化
l2_reg_lambda = 0.0
# 梯度阶段临界值
max_grad_norm = 5.0
# 全样本迭代次数
num_epochs = 1
# 训练集批样本大小
train_batch_size = 64
# 验证集批样本大小
eval_batch_size = 128
# 测试集批样本大小
test_batch_size = 128
# 迭代多少步保存一次模型文件
checkpoint_every = 1000
# 迭代多少步验证一次模型
eval_every_step = 1000
# 模型名称
model_name = bilstm



[THUC_NEWS]
data_path = /data/work/dl_project/data/corpus/thuc_news
label2idx_path = /data/work/dl_project/data/corpus/thuc_news/label2idx.json
pretrain_embedding =
output_path = /data/work/dl_project/data/model/thuc_news/bilstm_output
ckpt_model_path =
sequence_length = 512
num_labels = 14
embedding_dim = 300
vocab_size = 8000
hidden_sizes = [256]
is_training = True
dropout_keep_prob = 0.8
optimization = adam
learning_rate = 0.001
learning_decay_rate = 0.99
learning_decay_steps = 10000
l2_reg_lambda = 0.0
max_grad_norm = 5.0
num_epochs = 1
train_batch_size = 64
eval_batch_size = 128
test_batch_size = 128
eval_every_step = 500
model_name = bilstm
